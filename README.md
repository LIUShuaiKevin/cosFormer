# cosFormer

Official implementation of cosformer-attention in cosFormer: [Rethinking Softmax in Attention](https://arxiv.org/abs/2202.08791)

## Update log

- 2022/2/28
  - Add core code
- 
